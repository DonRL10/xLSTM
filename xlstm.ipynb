{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 512\n",
    "nembed = 64\n",
    "nembed2 = 64\n",
    "\n",
    "batch_size = 64\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(\"input.txt\", 'r').read()\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for ch, i in stoi.items()}\n",
    "encode = lambda x: [stoi[s] for s in x]\n",
    "decode = lambda x: \"\".join([itos[i] for i in x])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype = torch.long)\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data) - block_size - 1, (batch_size, ))\n",
    "    x = torch.stack([data[i: i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1: i + block_size + 1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            _, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "b, t, t = 2, 5, 5\n",
    "f = torch.randn(b, t)\n",
    "i = torch.randn(b, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2073, 0.6400, 0.5589, 0.7215, 0.1355],\n",
       "        [0.4899, 0.4087, 0.6986, 0.7709, 0.4859]])"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "F = torch.zeros(b, t, t)\n",
    "for l in range(t):\n",
    "    for j in range(t):\n",
    "        if j == l:\n",
    "            F[:, l, j] = torch.ones_like(F[:, l, j])\n",
    "        if l > j:\n",
    "            F[:, l, j] = f[:, j + 1: l + 1].sigmoid().prod(1)\n",
    "\n",
    "I = torch.zeros(b, t, t)\n",
    "for l in range(t):\n",
    "    for j in range(t):\n",
    "        if l >= j:\n",
    "            I[:, l, j] = i[:, j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.7311, 1.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5344, 0.7311, 1.0000, 0.0000, 0.0000],\n",
       "         [0.3907, 0.5344, 0.7311, 1.0000, 0.0000],\n",
       "         [0.2856, 0.3907, 0.5344, 0.7311, 1.0000]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.7311, 1.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5344, 0.7311, 1.0000, 0.0000, 0.0000],\n",
       "         [0.3907, 0.5344, 0.7311, 1.0000, 0.0000],\n",
       "         [0.2856, 0.3907, 0.5344, 0.7311, 1.0000]]])"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.7311, 1.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5344, 0.7311, 1.0000, 0.0000, 0.0000],\n",
       "         [0.3907, 0.5344, 0.7311, 1.0000, 0.0000],\n",
       "         [0.2856, 0.3907, 0.5344, 0.7311, 1.0000]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.7311, 1.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5344, 0.7311, 1.0000, 0.0000, 0.0000],\n",
       "         [0.3907, 0.5344, 0.7311, 1.0000, 0.0000],\n",
       "         [0.2856, 0.3907, 0.5344, 0.7311, 1.0000]]])"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = torch.nn.functional.pad(f[:,1:], (0, 1,), value = 1)\n",
    "F2 = (f[:, None, :].expand(-1, t, -1).sigmoid().tril(-1)  \n",
    "      + torch.ones(t, t)[None, :].triu()).flip(-1).cumprod(-1).flip(-1)\n",
    "F2.tril()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mLSTMCell(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.qkv= nn.Linear(nembed, nembed2 * 3)\n",
    "        self.fi = nn.Linear(nembed, 1 * 2)\n",
    "        self.o = nn.Linear(nembed, nembed2)\n",
    "    \n",
    "    def rnn_forward(self, xt, nprev, cprev):\n",
    "        q, k, v, = self.qkv(xt).split(nembed2, dim = -1) # b, t, c\n",
    "        i, f = self.fi(xt).split(1, dim = -1)\n",
    "        o = self.o(xt)\n",
    "        k = k * (nembed2 ** -0.5)\n",
    "        \n",
    "        f = torch.sigmoid(f)\n",
    "        i = torch.exp(i)\n",
    "        o = torch.sigmoid(o)\n",
    "        #cprev = b, c, c\n",
    "        #nprev = b, t, c\n",
    "\n",
    "        ct = f[..., None] * cprev + i[..., None] * (v[..., None] * k[..., None, :])\n",
    "        nt = f * nprev + i * k\n",
    "        ht_bar = ct @ q[..., None] / torch.abs(nt[..., None, :] @ q[..., None]).clamp(min = 1)\n",
    "        ht = o * ht_bar[..., -1]\n",
    "        return ht, nt, ct\n",
    "\n",
    "    def parallel_forward(self, xt):\n",
    "        b, t, c = xt.size()\n",
    "        q, k, v, = self.qkv(xt).split(nembed2, dim = -1) # b, t, c\n",
    "        i, f= self.fi(xt).split(1, dim = -1)\n",
    "        f = f.squeeze(-1)\n",
    "        i = i.squeeze(-1)\n",
    "        o = self.o(xt)\n",
    "        k = k * (nembed2 ** -0.5)\n",
    "\n",
    "        # F = torch.zeros(b, t, t).to(q.device)\n",
    "        # for l in range(t):\n",
    "        #     for j in range(t):\n",
    "        #         if j == l:\n",
    "        #             F[:, l, j] = torch.ones_like(F[:, l, j])\n",
    "        #         if l > j:\n",
    "        #             F[:, l, j] = f[:, j + 1: l + 1].sigmoid().prod(1)\n",
    "        f = torch.nn.functional.pad(f[:,1:], (0, 1,), value = 1)\n",
    "        F = (f[:, None, :].expand(-1, t, -1).sigmoid().tril(-1)  \n",
    "             + torch.ones(t, t)[None, :].triu().to(q.device)).flip(-1).cumprod(-1).flip(-1)\n",
    "        F = F.tril()\n",
    "        # I = torch.zeros(b, t, t).to(q.device)\n",
    "        # for l in range(t):\n",
    "        #     for j in range(t):\n",
    "        #         if l >= j:\n",
    "        #             I[:, l, j] = i[:, j]\n",
    "\n",
    "        I = torch.zeros(b, t, t).to(q.device)\n",
    "        I = torch.tril(I + i[:, None, :])\n",
    "        # print(f,'\\n',F, '\\n',I)\n",
    "        Cbar = (q @ k.transpose(-2 , -1)) * F * I.exp()\n",
    "        C = Cbar / torch.abs(Cbar.sum(2, keepdim = True)).clamp(min = 1)\n",
    "        hbar = C @ v\n",
    "        return hbar * o.sigmoid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0589,  0.0974,  0.2051, -0.0566,  0.0165, -0.0482,  0.1165,  0.0740,\n",
       "          0.0615,  0.1346, -0.0803, -0.0192, -0.1159,  0.2436, -0.1808,  0.0362,\n",
       "          0.0420, -0.0451,  0.0059, -0.0804, -0.1492, -0.2376, -0.0786, -0.1149,\n",
       "          0.0568, -0.0988,  0.0135, -0.0613,  0.0287,  0.1159,  0.0419, -0.1804,\n",
       "         -0.1795, -0.1460, -0.0021,  0.1443, -0.0487, -0.0275, -0.1300,  0.0016,\n",
       "          0.0272,  0.0595,  0.0927,  0.0931,  0.0313, -0.0498, -0.0746,  0.0177,\n",
       "          0.0545,  0.0658,  0.0346,  0.0038,  0.1204,  0.1313, -0.0329,  0.0241,\n",
       "          0.0091,  0.0742,  0.0119, -0.0508,  0.1329,  0.0128,  0.1369, -0.0510],\n",
       "        grad_fn=<SelectBackward0>),\n",
       " tensor([ 0.0589,  0.0974,  0.2051, -0.0566,  0.0165, -0.0482,  0.1165,  0.0740,\n",
       "          0.0615,  0.1346, -0.0803, -0.0192, -0.1159,  0.2436, -0.1808,  0.0362,\n",
       "          0.0420, -0.0451,  0.0059, -0.0804, -0.1492, -0.2376, -0.0786, -0.1149,\n",
       "          0.0568, -0.0988,  0.0135, -0.0613,  0.0287,  0.1159,  0.0419, -0.1804,\n",
       "         -0.1795, -0.1460, -0.0021,  0.1443, -0.0487, -0.0275, -0.1300,  0.0016,\n",
       "          0.0272,  0.0595,  0.0927,  0.0931,  0.0313, -0.0498, -0.0746,  0.0177,\n",
       "          0.0545,  0.0658,  0.0346,  0.0038,  0.1204,  0.1313, -0.0329,  0.0241,\n",
       "          0.0091,  0.0742,  0.0119, -0.0508,  0.1329,  0.0128,  0.1369, -0.0510],\n",
       "        grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xt = torch.randn(2, 5, 64)\n",
    "m = mLSTMCell()\n",
    "\n",
    "h = []\n",
    "cprev = torch.zeros(2, 1, 64, 64)\n",
    "nprev = torch.zeros(2, 1, 64)\n",
    "for t in range(5):\n",
    "    ht, nprev, cprev = m.rnn_forward(xt[:, t: t + 1], nprev, cprev)\n",
    "    h.append(ht)\n",
    "p1 = torch.cat(h, dim = 1)\n",
    "\n",
    "p2 = m.parallel_forward(xt)\n",
    "\n",
    "p2[0, -2], p1[0, -2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.start = nn.Parameter(torch.zeros(1, nembed2))\n",
    "        self.cstart = nn.Parameter(torch.zeros(1, nembed2, nembed2))\n",
    "        self.emb = nn.Embedding(vocab_size, nembed)\n",
    "        self.cell = mLSTMCell()\n",
    "        self.proj = nn.Linear(nembed2, vocab_size)\n",
    "    \n",
    "    def forward(self, idx, targets = None, tp = 0):\n",
    "        B, T = idx.size()\n",
    "        emb = self.emb(idx)\n",
    "        #sequentially iterate over inputs and update torche rnn state each tick\n",
    "        if tp:\n",
    "            nprev = self.start.expand((B, -1, -1))\n",
    "            cprev = self.cstart.expand((B, -1, -1, -1))\n",
    "            hiddens = []\n",
    "            for i in range(T):\n",
    "                xt = emb[:, i: i + 1, :]\n",
    "                # xt = nn.LayerNorm(nembed).cuda()(xt)\n",
    "                ht, nt, ct = self.cell.rnn_forward(xt, nprev, cprev)\n",
    "                nprev, cprev = nt, ct\n",
    "                hiddens.append(ht)\n",
    "            hiddens = torch.cat(hiddens, 1)\n",
    "        else:\n",
    "            hiddens = self.cell.parallel_forward(emb)\n",
    "        logits = self.proj(hiddens)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = torch.nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Parameters:  29315\n"
     ]
    }
   ],
   "source": [
    "model = RNN().to(device)\n",
    "print(\"Model Parameters: \", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 2.0031:  29%|██▉       | 874/3000 [01:08<02:47, 12.68it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m xb, yb \u001b[38;5;241m=\u001b[39m get_batch(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m model(xb, yb, tp \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\rohan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rohan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_iters = 3000\n",
    "eval_iters = 50\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 1e-3)\n",
    "\n",
    "from tqdm import tqdm\n",
    "pbar = tqdm(range(max_iters))\n",
    "\n",
    "for j in pbar:\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = model(xb, yb, tp = 0)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad(set_to_none = True)\n",
    "    pbar.set_description(f\"Loss {loss.item():.4f}\")\n",
    "    # if j % 50== 0:\n",
    "    #     losses = estimate_loss()\n",
    "    #     print(f\"\\nStep {j}: train_loss {losses['train']:.4f}  val_loss {losses['val']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 9.3108,  2.3435, -3.4952, -3.8625, -3.2206,  2.1256, -0.8196, -0.3650,\n",
       "         -1.7328,  1.2867, -1.0852, -3.1300, -1.4932,  5.9854,  5.6188,  5.6602,\n",
       "          4.8232,  3.5847,  5.1075,  5.1896,  5.6720,  5.9690,  2.7234,  4.1191,\n",
       "          4.9957,  5.5276,  4.8702,  4.7934,  4.3832,  3.1226,  3.3231,  5.3991,\n",
       "          6.6216,  2.5352,  2.8224,  6.4438, -3.6266,  4.2546, -4.9650, -1.6632,\n",
       "         -1.5166, -3.8289, -4.6490, -3.1766, -4.1552, -2.8709, -3.3555, -1.9243,\n",
       "         -2.7107, -4.1332, -1.3281, -3.1763, -3.2467, -3.7656, -0.8999, -4.5163,\n",
       "         -0.4250, -2.0757, -2.4817, -0.4103, -3.0978,  1.1716, -3.5617, -2.8300,\n",
       "         -4.9416], device='cuda:0', grad_fn=<SelectBackward0>),\n",
       " tensor([ 9.3108,  2.3435, -3.4952, -3.8625, -3.2207,  2.1256, -0.8196, -0.3650,\n",
       "         -1.7328,  1.2867, -1.0852, -3.1300, -1.4932,  5.9854,  5.6188,  5.6602,\n",
       "          4.8232,  3.5847,  5.1075,  5.1896,  5.6720,  5.9690,  2.7234,  4.1192,\n",
       "          4.9957,  5.5277,  4.8703,  4.7934,  4.3832,  3.1226,  3.3231,  5.3991,\n",
       "          6.6216,  2.5352,  2.8224,  6.4438, -3.6266,  4.2546, -4.9650, -1.6632,\n",
       "         -1.5166, -3.8289, -4.6490, -3.1766, -4.1552, -2.8709, -3.3555, -1.9243,\n",
       "         -2.7107, -4.1332, -1.3281, -3.1764, -3.2467, -3.7656, -0.8999, -4.5163,\n",
       "         -0.4250, -2.0757, -2.4817, -0.4103, -3.0978,  1.1716, -3.5617, -2.8300,\n",
       "         -4.9416], device='cuda:0', grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(xb)[0][2][6], model(xb, tp = 1)[0][2][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, idx, steps):\n",
    "    for _ in range(steps):\n",
    "        idx_cond = idx if idx.size(1) <= block_size else idx[:, -block_size:]\n",
    "        logits, _ = model(idx_cond, tp = 0)\n",
    "        logits = logits[:, -1, :]\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sour me fall mings be imener; ther's word thou comeses here his sous lash; love cren ounge and cliking kneys\n",
      "thou a alll sup lows\n",
      "Stall:\n",
      "Nod Ureces.\n",
      "\n",
      "QUEENCENVIG:\n",
      "Cown, prand he that Poucusterste;\n",
      "In dour truch theme, agrt for pocl'd me\n",
      "And ea thimbe? hoI earng.\n",
      "\n",
      "Ulore, onitelles mor ecent sily.\n",
      "\n",
      "DUKE TELOYBUENTCURY:\n",
      "The gones. BUre do?\n",
      "\n",
      "Goyst frole ben pirst cout mentwak forsiot thimente son, your me\n",
      "RICHARETIO:\n",
      "Comas dothizetrier's dite.\n",
      "Nlie one wopld pree whold.\n",
      "\n",
      "What me lock? EN:\n",
      "Buntll out your the din alld, cant gradthse a sunot buate.\n",
      "Wit theesesse,\n",
      "As oulde Whold for with sp ath the I not un de;\n",
      "WARYour depsuch\n",
      "LWINA:\n",
      "We!\n",
      "LONES Clay urcus tis of oftis fart them isell. ifes mose: thany silme:\n",
      "That come,\n",
      "Or rence ast quado? Th hyou I poode w with nechtid\n",
      "ith peady, Peed rown.\n",
      "\n",
      "Thonis the tword wild hus-deestat: for do ucas, breold,\n",
      "'lobdour the item;\n",
      "Thiria HEST:\n",
      "He thy willt I fil'd wast for Mit, itl guis yourfl\n",
      "For Arand page\n",
      "KE VI:\n",
      "He formm foolat, by armm beme of a for but f\n"
     ]
    }
   ],
   "source": [
    "print(decode((generate(model, torch.tensor([[0]], dtype = torch.long, device= 'cuda'), 1000))[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
