{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 8\n",
    "nembed = 64\n",
    "nembed2 = 64\n",
    "\n",
    "batch_size = 64\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(\"input.txt\", 'r').read()\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for ch, i in stoi.items()}\n",
    "encode = lambda x: [stoi[s] for s in x]\n",
    "decode = lambda x: \"\".join([itos[i] for i in x])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype = torch.long)\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data) - block_size - 1, (batch_size, ))\n",
    "    x = torch.stack([data[i: i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1: i + block_size + 1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            _, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mLSTMCell(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.qkv= nn.Linear(nembed, nembed2 * 3)\n",
    "        self.fi = nn.Linear(nembed, 1 * 2)\n",
    "        self.o = nn.Linear(nembed, nembed2)\n",
    "    \n",
    "    def rnn_forward(self, xt, nprev, cprev):\n",
    "        q, k, v, = self.qkv(xt).split(nembed2, dim = -1) # b, t, c\n",
    "        i, f = self.fi(xt).split(1, dim = -1)\n",
    "        o = self.o(xt)\n",
    "        k = k * (nembed2 ** -0.5)\n",
    "        \n",
    "        f = torch.sigmoid(f)\n",
    "        i = torch.exp(i)\n",
    "        o = torch.sigmoid(o)\n",
    "        #cprev = b, c, c\n",
    "        #nprev = b, t, c\n",
    "\n",
    "        ct = f[..., None] * cprev + i[..., None] * (v[..., None] * k[..., None, :])\n",
    "        nt = f * nprev + i * k\n",
    "        ht_bar = ct @ q[..., None] / torch.abs(nt[..., None, :] @ q[..., None]).clamp(min = 1)\n",
    "        ht = o * ht_bar[..., -1]\n",
    "        return ht, nt, ct\n",
    "\n",
    "    def parallel_forward(self, xt):\n",
    "        b, t, c = xt.size()\n",
    "        q, k, v, = self.qkv(xt).split(nembed2, dim = -1) # b, t, c\n",
    "        i, f= self.fi(xt).split(1, dim = -1)\n",
    "        f = f.squeeze(-1)\n",
    "        i = i.squeeze(-1)\n",
    "        o = self.o(xt)\n",
    "        k = k * (nembed2 ** -0.5)\n",
    "\n",
    "        F = torch.zeros(b, t, t).to(q.device)\n",
    "        for l in range(t):\n",
    "            for j in range(t):\n",
    "                if j == l:\n",
    "                    F[:, l, j] = torch.ones_like(F[:, l, j])\n",
    "                if l > j:\n",
    "                    F[:, l, j] = f[:, j + 1: l + 1].sigmoid().prod(1)\n",
    "        \n",
    "        I = torch.zeros(b, t, t).to(q.device)\n",
    "        for l in range(t):\n",
    "            for j in range(t):\n",
    "                if l >= j:\n",
    "                    I[:, l, j] = i[:, j]\n",
    "        # print(f,'\\n',F, '\\n',I)\n",
    "        Cbar = (q @ k.transpose(-2 , -1)) * F * I.exp()\n",
    "        C = Cbar / torch.abs(Cbar.sum(2, keepdim = True)).clamp(min = 1)\n",
    "        hbar = C @ v\n",
    "        return hbar * o.sigmoid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.start = nn.Parameter(torch.zeros(1, nembed2))\n",
    "        self.cstart = nn.Parameter(torch.zeros(1, nembed2, nembed2))\n",
    "        self.emb = nn.Embedding(vocab_size, nembed)\n",
    "        self.cell = mLSTMCell()\n",
    "        self.proj = nn.Linear(nembed2, vocab_size)\n",
    "    \n",
    "    def forward(self, idx, targets = None, tp = 0):\n",
    "        B, T = idx.size()\n",
    "        emb = self.emb(idx)\n",
    "        #sequentially iterate over inputs and update torche rnn state each tick\n",
    "        if tp:\n",
    "            nprev = self.start.expand((B, -1, -1))\n",
    "            cprev = self.cstart.expand((B, -1, -1, -1))\n",
    "            hiddens = []\n",
    "            for i in range(T):\n",
    "                xt = emb[:, i: i + 1, :]\n",
    "                # xt = nn.LayerNorm(nembed).cuda()(xt)\n",
    "                ht, nt, ct = self.cell.rnn_forward(xt, nprev, cprev)\n",
    "                nprev, cprev = nt, ct\n",
    "                hiddens.append(ht)\n",
    "            hiddens = torch.cat(hiddens, 1)\n",
    "        else:\n",
    "            hiddens = self.cell.parallel_forward(emb)\n",
    "        logits = self.proj(hiddens)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = torch.nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Parameters:  29315\n"
     ]
    }
   ],
   "source": [
    "model = RNN().to(device)\n",
    "print(\"Model Parameters: \", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters = 3000\n",
    "eval_iters = 50\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 1e-3)\n",
    "\n",
    "from tqdm import tqdm\n",
    "pbar = tqdm(range(max_iters))\n",
    "\n",
    "for j in pbar:\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = model(xb, yb, tp = 0)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad(set_to_none = True)\n",
    "    pbar.set_description(f\"Loss {loss.item():.4f}\")\n",
    "    # if j % 50== 0:\n",
    "    #     losses = estimate_loss()\n",
    "    #     print(f\"\\nStep {j}: train_loss {losses['train']:.4f}  val_loss {losses['val']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.3245,  3.5249,  0.0480, -3.0780, -3.5234,  0.5846,  1.5399, -1.2726,\n",
       "          0.8633, -3.1045,  0.3755,  0.1603, -1.2028, -3.2946, -4.1614, -3.6202,\n",
       "         -4.0068, -2.8602, -3.7481, -3.5890, -4.8860, -2.9233, -4.5777, -4.2971,\n",
       "         -3.7921, -4.0429, -3.4573, -3.0167, -4.5481, -3.8404, -3.1294, -3.6817,\n",
       "         -3.1601, -2.5103, -3.7956, -3.5982, -3.8480, -3.4387, -4.1663,  5.0372,\n",
       "         -2.8464, -0.7980,  0.1525,  5.7629, -2.5494, -1.6712,  1.0083,  4.5786,\n",
       "         -3.6007, -0.4976, -0.5019, -0.3955, -1.9380,  4.3374, -1.6218, -2.7170,\n",
       "          0.6193,  0.7554,  0.4953,  1.7111, -2.0904, -2.0211, -2.9157,  2.0636,\n",
       "         -3.3485], device='cuda:0', grad_fn=<SelectBackward0>),\n",
       " tensor([ 0.3245,  3.5249,  0.0480, -3.0780, -3.5234,  0.5846,  1.5399, -1.2726,\n",
       "          0.8633, -3.1045,  0.3755,  0.1603, -1.2028, -3.2946, -4.1614, -3.6202,\n",
       "         -4.0068, -2.8602, -3.7481, -3.5890, -4.8860, -2.9233, -4.5777, -4.2971,\n",
       "         -3.7921, -4.0429, -3.4573, -3.0167, -4.5481, -3.8404, -3.1294, -3.6817,\n",
       "         -3.1601, -2.5103, -3.7956, -3.5982, -3.8480, -3.4387, -4.1663,  5.0372,\n",
       "         -2.8464, -0.7980,  0.1525,  5.7629, -2.5494, -1.6712,  1.0083,  4.5786,\n",
       "         -3.6007, -0.4976, -0.5019, -0.3955, -1.9380,  4.3374, -1.6218, -2.7170,\n",
       "          0.6193,  0.7554,  0.4953,  1.7111, -2.0904, -2.0211, -2.9157,  2.0636,\n",
       "         -3.3485], device='cuda:0', grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(xb)[0][1][5], model(xb, tp = 1)[0][1][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, idx, max_new_tokens):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx if idx.size(1) <= block_size else idx[:, -block_size:]\n",
    "        logits, _ = model(idx_cond, tp = 1)\n",
    "        logits = logits[:, -1, :]\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Any asshreand hse uouln,'s my wis wertYEa wever, s asearttrust. teas, ver thato?\n",
      "To eswoungtot; fo o\n"
     ]
    }
   ],
   "source": [
    "print(decode((generate(model, torch.tensor([[0]], dtype = torch.long, device= 'cuda'), 100))[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
